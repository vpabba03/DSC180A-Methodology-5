Vishwak Pabba (vpabba@ucsd.edu)

Neural Network Compression with Error Guarantees
Alex Cloninger and Rayan Saab

1. **What is the most interesting topic covered in your domain this quarter?** The most interesting topic covered was how neural networks models can be quantized through uniquely efficient methods that all work well. Specifically, I really enjoyed learning about the GPFQ algorithm because it shows how using a simple alphabet of bits, you can reduce the size and essentially retain very similar performance to the original model.
2. **Describe a potential investigation you would like to pursue for your Quarter 2 Project.** A potential investigation for our Quarter 2 project is to perform experiments with a knowledge distillation method that takes advantage of the nueral tangent kernel. This method offers "lossless" compression per the researchers who designed it, however it has only be applied to DNNs and not CNNs, so we would like look for methods to apply these to CNNs.
3. **What is a potential change youâ€™d make to the approach taken in your current Quarter 1 Project?** I  think one potential change I would make to my approach taken in my Quarter 1 participation is to access more compute and memory resources. I was really hindered during the beginning of the project, and this was mainly due to a lack of utilizing my hardware resources fully.
4. **What other techniques would you be interested in using in your project?** In my project, I would be interested in using interesting visualizations that highlight how the model works. Maybe through some kind of iteraction on the website/ visualization, it would engage a visual demonstration of the model.
